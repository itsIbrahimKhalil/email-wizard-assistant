{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95205462",
   "metadata": {},
   "source": [
    "# Email Wizard Assistant - RAG Implementation Notebook\n",
    "#\n",
    "# This notebook details the development and testing of the core Retrieval-Augmented Generation (RAG) pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dde932",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "### Make sure you have activated your virtual environment and installed dependencies from `requirements.txt`.\n",
    "### ```bash\n",
    "### pip install -r requirements.txt\n",
    "### ```\n",
    "### For local execution involving the Gemini API, ensure the `GOOGLE_API_KEY` environment variable is set *before* starting Jupyter Lab/Notebook:\n",
    "### ```bash\n",
    "### export GOOGLE_API_KEY=\"YOUR_API_KEY\" # Linux/macOS\n",
    "### set GOOGLE_API_KEY=\"YOUR_API_KEY\"   # Windows CMD\n",
    "### $env:GOOGLE_API_KEY=\"YOUR_API_KEY\" # Windows PowerShell\n",
    "### ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a74804",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ibrah\\OneDrive\\Documents\\mytask\\email-wizard-assistant\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Embedding Model Library\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LLM Library (Google Gemini)\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030bc9ea",
   "metadata": {},
   "source": [
    "### 2. Configuration and Gemini Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774e6dfb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini client initialized successfully for model access (using configured key). Target model: gemini-2.5-flash-preview-04-17\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "# Use a model compatible with your google-genai setup, e.g., 'gemini-pro'\n",
    "GEMINI_MODEL_NAME = 'gemini-2.5-flash-preview-04-17'\n",
    "EMAIL_DATA_PATH = 'data/emails.json'\n",
    "EMBEDDING_SAVE_PATH = 'data/email_embeddings.npy'\n",
    "\n",
    "# --- Initialize Gemini Client ---\n",
    "GEMINI_CLIENT = None\n",
    "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"WARNING: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Gemini API calls will fail. Please set the environment variable and restart the kernel.\")\n",
    "else:\n",
    "    try:\n",
    "        # Using google-genai SDK client initialization\n",
    "        GEMINI_CLIENT = genai.Client(api_key=API_KEY)\n",
    "        print(f\"Gemini client initialized successfully for model access (using configured key). Target model: {GEMINI_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to initialize Gemini client: {e}\")\n",
    "        print(\"Please ensure your API key is valid and the environment variable is set correctly.\")\n",
    "        GEMINI_CLIENT = None # Ensure client is None if setup failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9675fa",
   "metadata": {},
   "source": [
    "### 3. Load and Prepare Email Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f3fa0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 50 emails from data/emails.json.\n",
      "   id                                             sender       date  \\\n",
      "0   1                   Chloe <chloe.g@emailfriends.com> 2024-05-10   \n",
      "1   2               Samantha Miller, Project Coordinator 2023-11-05   \n",
      "2   3                       tech_enthusiast_88@email.com 2024-01-20   \n",
      "3   4               Old Friend Mike <mikey.p@mymail.net> 2024-04-18   \n",
      "4   5  Project Phoenix Lead <phoenix.lead@corporate.com> 2024-02-15   \n",
      "\n",
      "                                     subject  \\\n",
      "0                       Italy Trip - Ideas?!   \n",
      "1         Meeting Request: Q4 Project Review   \n",
      "2     Inquiry about 'Aura Phone X1' Features   \n",
      "3                          Long time no see!   \n",
      "4  Project Phoenix - Phase 2 Progress Report   \n",
      "\n",
      "                                                body  \\\n",
      "0  Hey Liam,\\n\\nOMG, so excited we're actually do...   \n",
      "1  Dear Team,\\n\\nPlease let me know your availabi...   \n",
      "2  Hello OmniGadget Support,\\n\\nI'm interested in...   \n",
      "3  Hey Sarah,\\n\\nHow have you been? It feels like...   \n",
      "4  Team,\\n\\nThis email serves as an update on Pro...   \n",
      "\n",
      "                                           full_text  \n",
      "0  Sender: Chloe <chloe.g@emailfriends.com>\\n\\nIt...  \n",
      "1  Sender: Samantha Miller, Project Coordinator\\n...  \n",
      "2  Sender: tech_enthusiast_88@email.com\\n\\nInquir...  \n",
      "3  Sender: Old Friend Mike <mikey.p@mymail.net>\\n...  \n",
      "4  Sender: Project Phoenix Lead <phoenix.lead@cor...  \n"
     ]
    }
   ],
   "source": [
    "emails_df = None\n",
    "try:\n",
    "    emails_df = pd.read_json(EMAIL_DATA_PATH)\n",
    "    # Combine subject and body for embedding\n",
    "    emails_df['full_text'] = \"Sender: \" + emails_df['sender'] + \"\\n\\n\" + emails_df['subject'] + \"\\n\\n\" + emails_df['body']\n",
    "    print(f\"Successfully loaded {len(emails_df)} emails from {EMAIL_DATA_PATH}.\")\n",
    "    print(emails_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Email data file not found at {EMAIL_DATA_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load or process email data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6150d",
   "metadata": {},
   "source": [
    "### 4. Load Embedding Model and Embed Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d1e1db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformer model 'all-MiniLM-L6-v2' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Embedding Model ---\n",
    "sbert_model = None\n",
    "try:\n",
    "    sbert_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(f\"Sentence transformer model '{EMBEDDING_MODEL_NAME}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load sentence transformer model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2611c03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed embeddings from data/email_embeddings.npy...\n",
      "Loaded embeddings. Shape: (50, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- Embed Emails (Load if exists, otherwise generate and save) ---\n",
    "email_embeddings = np.array([]) # Initialize as empty\n",
    "\n",
    "if sbert_model is not None and emails_df is not None and not emails_df.empty:\n",
    "    if os.path.exists(EMBEDDING_SAVE_PATH):\n",
    "        try:\n",
    "            print(f\"Loading pre-computed embeddings from {EMBEDDING_SAVE_PATH}...\")\n",
    "            email_embeddings = np.load(EMBEDDING_SAVE_PATH)\n",
    "            print(f\"Loaded embeddings. Shape: {email_embeddings.shape}\")\n",
    "            if email_embeddings.shape[0] != len(emails_df):\n",
    "                print(\"WARNING: Number of embeddings does not match number of emails. Re-generating...\")\n",
    "                email_embeddings = np.array([]) # Force regeneration\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading embeddings: {e}. Will attempt to regenerate.\")\n",
    "            email_embeddings = np.array([]) # Force regeneration\n",
    "\n",
    "    if email_embeddings.size == 0: # If loading failed or file didn't exist\n",
    "        try:\n",
    "            print(f\"Generating embeddings for {len(emails_df)} email texts...\")\n",
    "            email_contents_to_embed = emails_df['full_text'].tolist()\n",
    "            email_embeddings = sbert_model.encode(email_contents_to_embed)\n",
    "            print(f\"Embeddings generated. Shape: {email_embeddings.shape}\")\n",
    "            # Save the generated embeddings\n",
    "            os.makedirs(os.path.dirname(EMBEDDING_SAVE_PATH), exist_ok=True) # Ensure data dir exists\n",
    "            np.save(EMBEDDING_SAVE_PATH, email_embeddings)\n",
    "            print(f\"Embeddings saved to {EMBEDDING_SAVE_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to generate or save embeddings: {e}\")\n",
    "            email_embeddings = np.array([]) # Ensure it's empty on failure\n",
    "else:\n",
    "    if sbert_model is None:\n",
    "        print(\"Skipping embedding generation: SBERT model not loaded.\")\n",
    "    if emails_df is None or emails_df.empty:\n",
    "        print(\"Skipping embedding generation: Email data not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9253d",
   "metadata": {},
   "source": [
    "### 5. Implement Similarity Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b31e85a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_emails(query, email_embeddings_db, all_emails_df, embedding_pipeline_model, top_n=3):\n",
    "    \"\"\"\n",
    "    Finds the top_n most similar emails to a given query using cosine similarity.\n",
    "    Assumes email_embeddings_db corresponds row-wise to all_emails_df.\n",
    "    \"\"\"\n",
    "    if embedding_pipeline_model is None:\n",
    "        print(\"ERROR: Embedding model not loaded for similarity search.\")\n",
    "        return [], []\n",
    "    if email_embeddings_db is None or email_embeddings_db.size == 0:\n",
    "        print(\"ERROR: Email embeddings not available for search.\")\n",
    "        return [], []\n",
    "    if all_emails_df is None or all_emails_df.empty:\n",
    "        print(\"ERROR: Email DataFrame not available for similarity search.\")\n",
    "        return [], []\n",
    "    if email_embeddings_db.shape[0] != len(all_emails_df):\n",
    "         print(\"ERROR: Mismatch between number of embeddings and emails.\")\n",
    "         return [], []\n",
    "\n",
    "\n",
    "    try:\n",
    "        query_embedding = embedding_pipeline_model.encode([query]) \n",
    "        similarities = cosine_similarity(query_embedding, email_embeddings_db)[0] \n",
    "\n",
    "        num_emails = email_embeddings_db.shape[0]\n",
    "        actual_top_n = min(top_n, num_emails)\n",
    "        if actual_top_n <= 0: return [], []\n",
    "\n",
    "        top_indices_sorted = np.argsort(similarities) \n",
    "        top_indices = top_indices_sorted[-actual_top_n:][::-1] \n",
    "\n",
    "        similar_emails_content = []\n",
    "        similar_email_details = []\n",
    "\n",
    "        for index in top_indices:\n",
    "            if 0 <= index < len(all_emails_df):\n",
    "                email_row = all_emails_df.iloc[index]\n",
    "                email_info = f\"Email ID: {email_row.get('id', 'N/A')}\\nSender: {email_row.get('sender', 'N/A')}\\nDate: {email_row.get('date', 'N/A')}\\nSubject: {email_row.get('subject', 'N/A')}\\nBody:\\n{email_row.get('body', 'N/A')}\"\n",
    "                similar_emails_content.append(email_info)\n",
    "                similar_email_details.append({\n",
    "                    \"id\": email_row.get('id', 'N/A'),\n",
    "                    \"similarity_score\": float(similarities[index]),\n",
    "                    \"subject\": email_row.get('subject', 'N/A')\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Index {index} out of bounds for emails_df.\")\n",
    "        return similar_emails_content, similar_email_details\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during similarity search: {e}\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94caf1ae",
   "metadata": {},
   "source": [
    "### 6. Implement Gemini Generation Function (using Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f429b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_email_response_gemini(user_query, retrieved_emails_content, stream_response=False):\n",
    "    \"\"\"\n",
    "    Generates a response using the initialized Gemini client based on the user query and retrieved emails.\n",
    "    \"\"\"\n",
    "    global GEMINI_CLIENT, GEMINI_MODEL_NAME # Access the globally configured client/model name\n",
    "\n",
    "    if GEMINI_CLIENT is None:\n",
    "        return \"ERROR: Gemini client not initialized. Cannot generate response.\"\n",
    "\n",
    "    if not retrieved_emails_content:\n",
    "        return \"I couldn't find any relevant past emails to answer your query. Could you please try rephrasing or provide more details?\"\n",
    "\n",
    "    email_context = \"\\n\\n---\\n\\n\".join(retrieved_emails_content)\n",
    "\n",
    "    prompt = f\"\"\"You are an Email Wizard Assistant. Your task is to answer the user's query based *only* on the provided email context below. Be concise and factual based on the emails. If the answer is not found in the emails, state that clearly. Do not make up information.\n",
    "\n",
    "Retrieved Email(s) Context:\n",
    "---\n",
    "{email_context}\n",
    "---\n",
    "\n",
    "User Query: \"{user_query}\"\n",
    "\n",
    "Assistant's Answer:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        if stream_response:\n",
    "             print(\"Note: Streaming response not fully implemented for simplified API/notebook testing here.\")\n",
    "             response = GEMINI_CLIENT.models.generate_content(\n",
    "                model=GEMINI_MODEL_NAME,\n",
    "                contents=prompt\n",
    "             )\n",
    "             return response.text if hasattr(response, 'text') else \"Streaming response (implementation needed).\"\n",
    "        else:\n",
    "            response = GEMINI_CLIENT.models.generate_content(\n",
    "                model=GEMINI_MODEL_NAME,\n",
    "                contents=prompt\n",
    "            )\n",
    "\n",
    "            prompt_feedback = getattr(response, 'prompt_feedback', None) # Safely get prompt_feedback or None\n",
    "            if prompt_feedback is not None:\n",
    "                block_reason = getattr(prompt_feedback, 'block_reason', None) # Safely get block_reason or None\n",
    "                if block_reason is not None: # Check if block_reason has a value (isn't None)\n",
    "                    reason_msg = getattr(prompt_feedback, 'block_reason_message', str(block_reason))\n",
    "                    print(f\"Warning: Gemini response blocked. Reason: {reason_msg}\")\n",
    "                    return f\"Sorry, the response generation was blocked due to safety filters ({reason_msg}).\"\n",
    "\n",
    "            if hasattr(response, 'text') and response.text:\n",
    "                return response.text.strip()\n",
    "            elif hasattr(response, 'parts') and response.parts:\n",
    "                # Concatenate text from parts if available\n",
    "                return \"\".join(part.text for part in response.parts if hasattr(part, 'text')).strip()\n",
    "            else:\n",
    "                 print(f\"Warning: Received empty or non-text response object, and prompt was not blocked: {response}\")\n",
    "                 return \"Sorry, I couldn't generate a valid response from the AI model based on the provided context.\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during Gemini API call: {type(e).__name__} - {e}\")\n",
    "        if \"429\" in str(e) or \"RESOURCE_EXHAUSTED\" in str(e):\n",
    "            return \"ERROR: The AI service is currently busy (Rate Limit Exceeded). Please wait and try again.\"\n",
    "        if \"API_KEY_INVALID\" in str(e) or \"PermissionDenied\" in str(type(e).__name__):\n",
    "             return \"ERROR: Authentication Error. Please check your API Key.\"\n",
    "        return f\"ERROR: An unexpected error occurred while contacting the AI service ({type(e).__name__}).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba405e44",
   "metadata": {},
   "source": [
    "### 7. Implement Full RAG Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eecef75a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def rag_email_assistant(user_query, email_embeddings_db, all_emails_df, embedding_pipeline_model, llm_generation_function, top_n_retrieval=3):\n",
    "    \"\"\"\n",
    "    Orchestrates the RAG pipeline: retrieve similar emails and generate response.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing query: '{user_query}'\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Retrieve relevant emails\n",
    "    print(f\"Step 1: Finding top {top_n_retrieval} similar emails...\")\n",
    "    retrieved_email_snippets, retrieved_details = find_similar_emails(\n",
    "        user_query,\n",
    "        email_embeddings_db,\n",
    "        all_emails_df,\n",
    "        embedding_pipeline_model,\n",
    "        top_n=top_n_retrieval\n",
    "    )\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "    if not retrieved_email_snippets:\n",
    "        print(f\"Step 1 Result: No relevant emails found. (Took {retrieval_time:.2f}s)\")\n",
    "        return \"I couldn't find any relevant past emails to answer your query. Could you please try rephrasing or provide more details?\"\n",
    "\n",
    "    print(f\"Step 1 Result: Retrieved {len(retrieved_email_snippets)} email snippets. (Took {retrieval_time:.2f}s)\")\n",
    "    print(\"Retrieved Email Subjects & Scores:\")\n",
    "    for detail in retrieved_details:\n",
    "        print(f\"  - ID: {detail['id']}, Score: {detail['similarity_score']:.4f}, Subject: {detail['subject']}\")\n",
    "\n",
    "\n",
    "    # 2. Generate response using LLM with retrieved context\n",
    "    print(\"\\nStep 2: Generating response with Gemini using retrieved context...\")\n",
    "    generation_start_time = time.time()\n",
    "    generated_response = llm_generation_function(\n",
    "        user_query,\n",
    "        retrieved_email_snippets,\n",
    "        stream_response=False # Keep simple for notebook test\n",
    "    )\n",
    "    generation_time = time.time() - generation_start_time\n",
    "    print(f\"Step 2 Result: Response generated. (Took {generation_time:.2f}s)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.2f}s\")\n",
    "    return generated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7d701",
   "metadata": {},
   "source": [
    "### 8. Test the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71c64128",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing RAG Email Assistant ---\n",
      "\n",
      "Processing query: 'What's the status of Project Phoenix?'\n",
      "Step 1: Finding top 3 similar emails...\n",
      "Step 1 Result: Retrieved 3 email snippets. (Took 0.02s)\n",
      "Retrieved Email Subjects & Scores:\n",
      "  - ID: 26, Score: 0.4931, Subject: Meeting Minutes: Project Skyward Strategy Session - 2023-09-19\n",
      "  - ID: 5, Score: 0.4878, Subject: Project Phoenix - Phase 2 Progress Report\n",
      "  - ID: 2, Score: 0.4553, Subject: Meeting Request: Q4 Project Review\n",
      "\n",
      "Step 2: Generating response with Gemini using retrieved context...\n",
      "Step 2 Result: Response generated. (Took 3.00s)\n",
      "\n",
      "Total processing time: 3.02s\n",
      "\n",
      "Email Wizard's Assistant Reply 1:\n",
      "==============================\n",
      "Based on the emails provided:\n",
      "\n",
      "The initial data migration for Project Phoenix Phase 2 has been successfully completed. The development team is currently implementing new user interface modules. Testing for Module A is scheduled to begin next Monday and is anticipated to take approximately one week. The design team has finalized mockups for Module B and shared them with stakeholders for feedback. The project is currently on track with the revised timeline.\n",
      "==============================\n",
      "\n",
      "Processing query: 'Inquiries about Aura Phone X1'\n",
      "Step 1: Finding top 2 similar emails...\n",
      "Step 1 Result: Retrieved 2 email snippets. (Took 0.01s)\n",
      "Retrieved Email Subjects & Scores:\n",
      "  - ID: 3, Score: 0.6808, Subject: Inquiry about 'Aura Phone X1' Features\n",
      "  - ID: 5, Score: 0.2639, Subject: Project Phoenix - Phase 2 Progress Report\n",
      "\n",
      "Step 2: Generating response with Gemini using retrieved context...\n",
      "Step 2 Result: Response generated. (Took 1.65s)\n",
      "\n",
      "Total processing time: 1.66s\n",
      "\n",
      "Email Wizard's Assistant Reply 2:\n",
      "==============================\n",
      "Based on the emails, the inquiries about the Aura Phone X1 are regarding its battery life under typical usage conditions and whether the camera supports RAW image capture.\n",
      "==============================\n",
      "\n",
      "Processing query: 'Any emails regarding a dog which got lost?'\n",
      "Step 1: Finding top 3 similar emails...\n",
      "Step 1 Result: Retrieved 3 email snippets. (Took 0.01s)\n",
      "Retrieved Email Subjects & Scores:\n",
      "  - ID: 6, Score: 0.5776, Subject: LOST DOG: 'Buddy' - Golden Retriever - Maple Street Area\n",
      "  - ID: 13, Score: 0.3542, Subject: Some fluffy news!\n",
      "  - ID: 15, Score: 0.2942, Subject: Question about Summer Street Fair\n",
      "\n",
      "Step 2: Generating response with Gemini using retrieved context...\n",
      "Step 2 Result: Response generated. (Took 2.67s)\n",
      "\n",
      "Total processing time: 2.67s\n",
      "\n",
      "Email Wizard's Assistant Reply 3:\n",
      "==============================\n",
      "Yes, there is one email regarding a lost dog.\n",
      "\n",
      "Email ID 6 from Worried Neighbor reports that their Golden Retriever, named Buddy, went missing on December 1st near Maple Street and Oak Avenue. Buddy is friendly, has a red collar with a tag, answers to his name, is about 70 lbs with light golden fur, and was last seen heading towards the park. The contact number provided is 555-123-4567.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "all_components_ready = (\n",
    "    'GEMINI_CLIENT' in globals() and GEMINI_CLIENT is not None and\n",
    "    'email_embeddings' in globals() and isinstance(email_embeddings, np.ndarray) and email_embeddings.size > 0 and\n",
    "    'emails_df' in globals() and isinstance(emails_df, pd.DataFrame) and not emails_df.empty and\n",
    "    'sbert_model' in globals() and sbert_model is not None\n",
    ")\n",
    "\n",
    "if all_components_ready:\n",
    "    print(\"\\n--- Testing RAG Email Assistant ---\")\n",
    "\n",
    "    # Test Query 1 (Based on provided dataset)\n",
    "    query1 = \"What's the status of Project Phoenix?\"\n",
    "    assistant_reply1 = rag_email_assistant(\n",
    "        user_query=query1,\n",
    "        email_embeddings_db=email_embeddings,\n",
    "        all_emails_df=emails_df,\n",
    "        embedding_pipeline_model=sbert_model,\n",
    "        llm_generation_function=generate_email_response_gemini\n",
    "    )\n",
    "    print(\"\\nEmail Wizard's Assistant Reply 1:\")\n",
    "    print(\"=\"*30)\n",
    "    print(assistant_reply1)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    # Test Query 2 (Based on provided dataset)\n",
    "    query2 = \"Inquiries about Aura Phone X1\"\n",
    "    assistant_reply2 = rag_email_assistant(\n",
    "        user_query=query2,\n",
    "        email_embeddings_db=email_embeddings,\n",
    "        all_emails_df=emails_df,\n",
    "        embedding_pipeline_model=sbert_model,\n",
    "        llm_generation_function=generate_email_response_gemini,\n",
    "        top_n_retrieval=2 # Retrieve fewer docs for this one maybe\n",
    "    )\n",
    "    print(\"\\nEmail Wizard's Assistant Reply 2:\")\n",
    "    print(\"=\"*30)\n",
    "    print(assistant_reply2)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "     # Test Query 3 (Based on provided dataset)\n",
    "    query3 = \"Any emails regarding a dog which got lost?\"\n",
    "    assistant_reply3 = rag_email_assistant(\n",
    "        user_query=query3,\n",
    "        email_embeddings_db=email_embeddings,\n",
    "        all_emails_df=emails_df,\n",
    "        embedding_pipeline_model=sbert_model,\n",
    "        llm_generation_function=generate_email_response_gemini\n",
    "    )\n",
    "    print(\"\\nEmail Wizard's Assistant Reply 3:\")\n",
    "    print(\"=\"*30)\n",
    "    print(assistant_reply3)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- RAG Test Skipped ---\")\n",
    "    print(\"Reason: Not all required components (Gemini client, embeddings, data, models) were initialized successfully.\")\n",
    "    print(f\"- Gemini Client Ready: {'Yes' if 'GEMINI_CLIENT' in globals() and GEMINI_CLIENT is not None else 'No'}\")\n",
    "    print(f\"- Embeddings Ready: {'Yes' if 'email_embeddings' in globals() and isinstance(email_embeddings, np.ndarray) and email_embeddings.size > 0 else 'No'}\")\n",
    "    print(f\"- DataFrame Ready: {'Yes' if 'emails_df' in globals() and isinstance(emails_df, pd.DataFrame) and not emails_df.empty else 'No'}\")\n",
    "    print(f\"- SBERT Model Ready: {'Yes' if 'sbert_model' in globals() and sbert_model is not None else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74bc7d",
   "metadata": {},
   "source": [
    "### 9. Evaluation\n",
    "\n",
    "This section evaluates the performance of the Email Wizard Assistant based on the RAG pipeline implemented above. We will look at search speed, accuracy of similarity, and coherence of responses using the test queries executed.\n",
    "\n",
    "#### 9.1. Search Speed\n",
    "\n",
    "The similarity search speed measures the time taken to embed the user's query, compare it against the pre-computed email embeddings, and retrieve the top N most similar emails. This is primarily the execution time of the `find_similar_emails` function.\n",
    "\n",
    "Based on the test runs:\n",
    "*   **Query 1 (\"What's the status of Project Phoenix?\"):** Retrieval (Step 1) took approximately **0.02 seconds**.\n",
    "*   **Query 2 (\"Inquiries about Aura Phone X1\"):** Retrieval (Step 1) took approximately **0.01 seconds**.\n",
    "*   **Query 3 (\"Any emails regarding a dog which got lost?\"):** Retrieval (Step 1) took approximately **0.01 seconds**.\n",
    "\n",
    "**Conclusion on Search Speed:**\n",
    "For the current dataset of 50 emails, using `all-MiniLM-L6-v2` embeddings and scikit-learn's `cosine_similarity` for exact search, the retrieval times are extremely low (in the order of tens of milliseconds). This indicates that the current search mechanism is highly efficient and more than adequate for real-time interaction with this dataset size.\n",
    "\n",
    "#### 9.2. Accuracy of Similarity (Qualitative Assessment)\n",
    "\n",
    "This metric assesses how relevant the emails retrieved by the `find_similar_emails` function are to the user's query. This is a qualitative assessment based on the `Retrieved Email Subjects & Scores` printed during the RAG test calls.\n",
    "\n",
    "*   **Query 1: \"What's the status of Project Phoenix?\"**\n",
    "    *   **Retrieved:**\n",
    "        *   ID 26, Score: 0.4931, Subject: Meeting Minutes: Project Skyward Strategy Session - 2023-09-19\n",
    "        *   ID 5, Score: 0.4878, Subject: Project Phoenix - Phase 2 Progress Report\n",
    "        *   ID 2, Score: 0.4553, Subject: Meeting Request: Q4 Project Review\n",
    "    *   **Assessment:** Good retrieval. The system successfully identified the most critical email (ID 5: \"Project Phoenix - Phase 2 Progress Report\") as a top result. The other emails (ID 26 and ID 2) have thematic overlaps concerning projects and meetings, which is understandable for semantic search. The retrieval of ID 5 provides the necessary context.\n",
    "\n",
    "*   **Query 2: \"Inquiries about Aura Phone X1\"**\n",
    "    *   **Retrieved:**\n",
    "        *   ID 3, Score: 0.6808, Subject: Inquiry about 'Aura Phone X1' Features\n",
    "        *   ID 5, Score: 0.2639, Subject: Project Phoenix - Phase 2 Progress Report\n",
    "    *   **Assessment:** Excellent retrieval for the primary document. Email ID 3, which is directly about the 'Aura Phone X1', was retrieved with a high similarity score (0.6808). The second retrieved email (ID 5) is less relevant but was likely picked up due to general \"project\" or \"feature\" related terms if the query implies technical aspects. The key success is retrieving ID 3.\n",
    "\n",
    "*   **Query 3: \"Any emails regarding a dog which got lost?\"**\n",
    "    *   **Retrieved:**\n",
    "        *   ID 6, Score: 0.5776, Subject: LOST DOG: 'Buddy' - Golden Retriever - Maple Street Area\n",
    "        *   ID 13, Score: 0.3542, Subject: Some fluffy news!\n",
    "        *   ID 15, Score: 0.2942, Subject: Question about Summer Street Fair\n",
    "    *   **Assessment:** Very good retrieval. The most relevant email (ID 6: \"LOST DOG: 'Buddy'\") was identified with the highest score. Email ID 13 (\"Some fluffy news!\") about adopting a kitten was also retrieved, likely due to the semantic similarity of \"fluffy\" and general pet-related topics to \"dog.\" Email ID 15 is less relevant but might have been pulled due to general \"question\" or \"community event\" themes if the \"lost dog\" email also had community aspects. The primary success is the retrieval of ID 6.\n",
    "\n",
    "**Conclusion on Accuracy of Similarity:**\n",
    "The similarity search demonstrates good performance in retrieving semantically relevant documents for various query types. It successfully identifies emails directly related to the query's main subject with high similarity scores. It also occasionally retrieves emails with broader thematic overlaps, which is a natural characteristic of semantic search. Overall, it provides a strong foundation for the RAG pipeline by supplying relevant context.\n",
    "\n",
    "#### 9.3. Coherence of Responses (Qualitative Assessment)\n",
    "\n",
    "This metric evaluates the quality of the final answer generated by the Gemini model, considering its clarity, relevance to the query, and factual grounding in the retrieved email context.\n",
    "\n",
    "*   **Reply 1 (Project Phoenix):**\n",
    "    *   *Generated Text:* \"Based on the emails provided:\\n\\nThe initial data migration for Project Phoenix Phase 2 has been successfully completed. The development team is currently implementing new user interface modules. Testing for Module A is scheduled to begin next Monday and is anticipated to take approximately one week. The design team has finalized mockups for Module B and shared them with stakeholders for feedback. The project is currently on track with the revised timeline.\"\n",
    "    *   **Assessment:** **Excellent.** The response is coherent, directly answers the query, and accurately synthesizes information from the retrieved context (primarily Email ID 5).\n",
    "\n",
    "*   **Reply 2 (Aura Phone X1):**\n",
    "    *   *Generated Text:* \"Based on the emails, the inquiries about the Aura Phone X1 are regarding its battery life under typical usage conditions and whether the camera supports RAW image capture.\"\n",
    "    *   **Assessment:** **Excellent.** The response is coherent and accurately extracts the specific inquiries mentioned in the relevant retrieved email (ID 3).\n",
    "\n",
    "*   **Reply 3 (Lost Dog):**\n",
    "    *   *Generated Text:* \"Yes, there is one email regarding a lost dog.\\n\\nEmail ID 6 from Worried Neighbor reports that their Golden Retriever, named Buddy, went missing on December 1st near Maple Street and Oak Avenue. Buddy is friendly, has a red collar with a tag, answers to his name, is about 70 lbs with light golden fur, and was last seen heading towards the park. The contact number provided is 555-123-4567.\"\n",
    "    *   **Assessment:** **Excellent.** The response directly answers the question, correctly identifies the relevant email (ID 6), and accurately summarizes all the key details about the lost dog from that email.\n",
    "\n",
    "**Conclusion on Coherence of Responses:**\n",
    "The Gemini model consistently generates coherent, accurate, and contextually appropriate responses. It effectively leverages the retrieved email snippets to answer user queries factually and concisely. The responses are well-grounded in the provided context and demonstrate good summarization and information extraction capabilities.\n",
    "\n",
    "#### 9.4. Overall Evaluation Summary\n",
    "\n",
    "The RAG-based Email Wizard Assistant effectively integrates retrieval and generation to answer user queries based on a sample email dataset.\n",
    "*   **Search Speed:** The similarity search is very fast and suitable for this dataset size.\n",
    "*   **Retrieval Accuracy:** The system generally retrieves highly relevant documents for user queries, demonstrating good semantic understanding.\n",
    "*   **Response Coherence:** The generated responses are consistently coherent, factually accurate based on the retrieved context, and directly address the user's questions.\n",
    "\n",
    "The system performs well on the given tasks. For larger and more complex datasets, further optimization of retrieval (e.g., ANN) and continued prompt engineering would be beneficial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
